{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lxml.html\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'text_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f8449508332a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_html\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#text_content()メソッドはそのタグ以下にあるすべてのテキストを取得する\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcssselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'#news_body > p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'text_content'"
     ]
    }
   ],
   "source": [
    "target_url = 'http://news.tv-asahi.co.jp/news_politics/articles/000041338.html'\n",
    "target_html = requests.get(target_url).text\n",
    "root = lxml.html.fromstring(target_html)\n",
    "#text_content()メソッドはそのタグ以下にあるすべてのテキストを取得する\n",
    "root.cssselect('#news_body > p').text_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h3 class=\"tit\">着用タグからコーディネートを探す</h3>\n",
      "着用タグからコーディネートを探す\n"
     ]
    }
   ],
   "source": [
    "# coding: UTF-8\n",
    "import urllib.request as urllibr\n",
    "import urllib.error as urllibe\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# アクセスするURL\n",
    "url = \"https://wear.jp/komasan1106/13455646/\"\n",
    "\n",
    "# URLにアクセスする htmlが帰ってくる → <html><head><title>経済、株価、ビジネス、政治のニュース:日経電子版</title></head><body....\n",
    "html = urllibr.urlopen(url)\n",
    "\n",
    "# htmlをBeautifulSoupで扱う\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# タイトル要素を取得する → <title>経済、株価、ビジネス、政治のニュース:日経電子版</title>\n",
    "title_tag = soup.find(\"h3\",attrs={\"class\":\"tit\"})\n",
    "\n",
    "# 要素の文字列を取得する → 経済、株価、ビジネス、政治のニュース:日経電子版\n",
    "title = title_tag.string\n",
    "\n",
    "# タイトル要素を出力\n",
    "print(title_tag)\n",
    "\n",
    "# タイトルを文字列を出力\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-67-19d871b6f018>, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-67-19d871b6f018>\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    a.decompose():\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "#抽出してきたタグを含む文字列から必要な単語のみをとってくる関数\n",
    "def str_select(tag_name):\n",
    "    str_list = []\n",
    "    for name in tag_name:\n",
    "        str_list.append('{name}'.format(name=name.string))\n",
    "    return str_list\n",
    "\n",
    "#WEBページのHTMLをgetし、ファイルとして保存\n",
    "res = requests.get('https://wear.jp/komasan1106/13455646/') #Response オブジェクト\n",
    "with open('wear-komasan1106.html', 'wb') as f:\n",
    "    f.write(res.content)\n",
    "\n",
    "#WEBページの文字コードを確認して、encoding指定してHTMLを読み込む\n",
    "html = open('wear-komasan1106.html',encoding='UTF-8').read()\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "\n",
    "#欲しい情報が構成されているHTMLタグを指定して、データをとってくる\n",
    "#tag名\n",
    "tags = soup.findAll('ul', attrs={\"class\":\"clearfix\"})\n",
    "# print(tags)\n",
    "tag_list = []\n",
    "for tag in tags:\n",
    "    for li in tag.findAll('li'):\n",
    "        a = li.find('a')\n",
    "        a.decompose():\n",
    "        tag_list.append(a)\n",
    "        \n",
    "print(tag_list)\n",
    "#         print(\"%s's url is %s\" % (a.text, a.attrs['href']))\n",
    "\n",
    "# for text in soup.find_all(text=True):\n",
    "#     if text.strip():\n",
    "#         print(text)\n",
    "    \n",
    "    \n",
    "    \n",
    "# # #とってきたタグを含む文字列から欲しい単語のみを抽出\n",
    "# # tag_list = str_select(tag)\n",
    "# # pref_list = str_select(pref)\n",
    "# rank_list = []\n",
    "# for i in range(1,len(tag_list)+1):\n",
    "#     rank_list.append(i)\n",
    "\n",
    "# #結果をデータフレーム化\n",
    "# output = pd.DataFrame()\n",
    "# output[\"tag\"] = tag_list\n",
    "# # output[\"pref\"] = pref_list\n",
    "# output[\"rank\"] = rank_list\n",
    "\n",
    "# #出力\n",
    "# output.to_csv('meigara_pref_rank.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
